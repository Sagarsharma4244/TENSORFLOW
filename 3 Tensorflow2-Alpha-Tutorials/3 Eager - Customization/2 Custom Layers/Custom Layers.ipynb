{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Layers\n",
    "Source: https://www.tensorflow.org/alpha/tutorials/eager/custom_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow==2.0.0-alpha0\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers: common sets of useful operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the tf.keras.layers package, layers are objects. To construct a layer,\n",
    "# simply construct the object. Most layers take as a first argument the number\n",
    "# of output dimensions / channels.\n",
    "layer = tf.keras.layers.Dense(100)\n",
    "# The number of input dimensions is often unnecessary, as it can be inferred\n",
    "# the first time the layer is used, but it can be provided if you want to \n",
    "# specify it manually, which is useful in some complex models.\n",
    "layer = tf.keras.layers.Dense(10, input_shape=(None, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full list of pre-existing layers can be seen in the documentation. It includes Dense (a fully-connected layer), Conv2D, LSTM, BatchNormalization, Dropout, and many others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=29, shape=(10, 10), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To use a layer, simply call it.\n",
    "layer(tf.zeros([10, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_1/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
       " array([[-0.2590437 ,  0.21691412, -0.45973426, -0.533728  , -0.4305027 ,\n",
       "          0.09812999, -0.27044576, -0.257358  ,  0.0051018 ,  0.39474005],\n",
       "        [ 0.1851781 ,  0.11184257,  0.03325564, -0.58809835, -0.35753268,\n",
       "          0.3647654 , -0.5471086 ,  0.35068005,  0.4582886 ,  0.35093552],\n",
       "        [ 0.08469677,  0.1407538 , -0.60918665,  0.48853916, -0.569168  ,\n",
       "          0.5360412 ,  0.6251305 ,  0.33964157,  0.5688638 , -0.30087864],\n",
       "        [-0.11585826,  0.5057865 , -0.13786426,  0.17978483, -0.56006783,\n",
       "         -0.50721097,  0.4939235 ,  0.41439885, -0.36684904,  0.12543094],\n",
       "        [-0.04391342, -0.52856046, -0.5648074 ,  0.19395542,  0.10447741,\n",
       "          0.12701923, -0.5590389 ,  0.01395297, -0.12737793,  0.39886147]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Layers have many useful methods. For example, you can inspect all variables\n",
    "# in a layer using `layer.variables` and trainable variables using \n",
    "# `layer.trainable_variables`. In this case a fully-connected layer\n",
    "# will have variables for weights and biases.\n",
    "layer.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'dense_1/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
       " array([[-0.2590437 ,  0.21691412, -0.45973426, -0.533728  , -0.4305027 ,\n",
       "          0.09812999, -0.27044576, -0.257358  ,  0.0051018 ,  0.39474005],\n",
       "        [ 0.1851781 ,  0.11184257,  0.03325564, -0.58809835, -0.35753268,\n",
       "          0.3647654 , -0.5471086 ,  0.35068005,  0.4582886 ,  0.35093552],\n",
       "        [ 0.08469677,  0.1407538 , -0.60918665,  0.48853916, -0.569168  ,\n",
       "          0.5360412 ,  0.6251305 ,  0.33964157,  0.5688638 , -0.30087864],\n",
       "        [-0.11585826,  0.5057865 , -0.13786426,  0.17978483, -0.56006783,\n",
       "         -0.50721097,  0.4939235 ,  0.41439885, -0.36684904,  0.12543094],\n",
       "        [-0.04391342, -0.52856046, -0.5648074 ,  0.19395542,  0.10447741,\n",
       "          0.12701923, -0.5590389 ,  0.01395297, -0.12737793,  0.39886147]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The variables are also accessible through nice accessors\n",
    "layer.kernel, layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing custom layers\n",
    "\n",
    "The best way to implement your own layer is extending the tf.keras.Layer class and implementing: \n",
    "\n",
    "- ` __init__` , where you can do all input-independent initialization \n",
    "- `build`, where you know the shapes of the input tensors and can do the rest of the initialization \n",
    "- `call`, where you do the forward computation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n",
      "[<tf.Variable 'my_dense_layer/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
      "array([[ 0.03631818, -0.58750993,  0.10124737, -0.11126506,  0.52455   ,\n",
      "         0.03279692,  0.2219826 ,  0.4701069 , -0.33879772, -0.01316237],\n",
      "       [-0.5861447 ,  0.01428092, -0.21908426, -0.6169046 ,  0.26401162,\n",
      "         0.18212461,  0.51165444,  0.41104776, -0.333844  ,  0.09821153],\n",
      "       [ 0.02781546,  0.22092885,  0.59590393, -0.45599303,  0.18668795,\n",
      "         0.4264421 ,  0.3472113 ,  0.5737639 , -0.370194  ,  0.13730425],\n",
      "       [ 0.01770127,  0.5604488 ,  0.33765596,  0.3793909 , -0.10074466,\n",
      "        -0.2970694 ,  0.21532738,  0.42780143,  0.35857612, -0.15994883],\n",
      "       [-0.03323889,  0.60615605,  0.4721591 , -0.09371561, -0.26866177,\n",
      "        -0.21266398,  0.06373674, -0.19423798,  0.1280387 , -0.47377843]],\n",
      "      dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_outputs):\n",
    "    super(MyDenseLayer, self).__init__()\n",
    "    self.num_outputs = num_outputs\n",
    "    \n",
    "  def build(self, input_shape):\n",
    "    self.kernel = self.add_variable(\"kernel\", \n",
    "                                    shape=[int(input_shape[-1]), \n",
    "                                           self.num_outputs])\n",
    "    \n",
    "  def call(self, input):\n",
    "    return tf.matmul(input, self.kernel)\n",
    "  \n",
    "layer = MyDenseLayer(10)\n",
    "print(layer(tf.zeros([10, 5])))\n",
    "print(layer.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you don't have to wait until build is called to create your variables, you can also create them in __init__.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models: composing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]]], shape=(1, 2, 3, 3), dtype=float32)\n",
      "['resnet_identity_block/conv2d/kernel:0', 'resnet_identity_block/conv2d/bias:0', 'resnet_identity_block/batch_normalization_v2/gamma:0', 'resnet_identity_block/batch_normalization_v2/beta:0', 'resnet_identity_block/conv2d_1/kernel:0', 'resnet_identity_block/conv2d_1/bias:0', 'resnet_identity_block/batch_normalization_v2_1/gamma:0', 'resnet_identity_block/batch_normalization_v2_1/beta:0', 'resnet_identity_block/conv2d_2/kernel:0', 'resnet_identity_block/conv2d_2/bias:0', 'resnet_identity_block/batch_normalization_v2_2/gamma:0', 'resnet_identity_block/batch_normalization_v2_2/beta:0']\n"
     ]
    }
   ],
   "source": [
    "class ResnetIdentityBlock(tf.keras.Model):\n",
    "  def __init__(self, kernel_size, filters):\n",
    "    super(ResnetIdentityBlock, self).__init__(name='')\n",
    "    filters1, filters2, filters3 = filters\n",
    "\n",
    "    self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1))\n",
    "    self.bn2a = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size, padding='same')\n",
    "    self.bn2b = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1))\n",
    "    self.bn2c = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "  def call(self, input_tensor, training=False):\n",
    "    x = self.conv2a(input_tensor)\n",
    "    x = self.bn2a(x, training=training)\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "    x = self.conv2b(x)\n",
    "    x = self.bn2b(x, training=training)\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "    x = self.conv2c(x)\n",
    "    x = self.bn2c(x, training=training)\n",
    "\n",
    "    x += input_tensor\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "    \n",
    "block = ResnetIdentityBlock(1, [1, 2, 3])\n",
    "print(block(tf.zeros([1, 2, 3, 3])))\n",
    "print([x.name for x in block.trainable_variables])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of the time, however, models which compose many layers simply call one layer after the other. This can be done in very little code using `tf.keras.Sequential`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=726, shape=(1, 2, 3, 3), dtype=float32, numpy=\n",
       "array([[[[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " my_seq = tf.keras.Sequential([tf.keras.layers.Conv2D(1, (1, 1), \n",
    "                                                      input_shape=(\n",
    "                                                          None, None, 3)),\n",
    "                               tf.keras.layers.BatchNormalization(),\n",
    "                               tf.keras.layers.Conv2D(2, 1,\n",
    "                                                      padding='same'),\n",
    "                               tf.keras.layers.BatchNormalization(),\n",
    "                               tf.keras.layers.Conv2D(3, (1, 1)),\n",
    "                               tf.keras.layers.BatchNormalization()])\n",
    "my_seq(tf.zeros([1, 2, 3, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
