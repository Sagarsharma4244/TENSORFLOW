{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation with TensorFlow Keras\n",
    "Source: https://www.tensorflow.org/alpha/guide/keras/training_and_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide covers training, evaluation, and prediction (inference) models in TensorFlow 2.0 in two broad situations:\n",
    "\n",
    "- Using build-in training & evaluation loops\n",
    "- Writing your own training & evaluation loops from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Using build-in training & evaluation loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API overview: a first end-to-end example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape:  (60000, 28, 28)\n",
      "y_tain.shape:  (60000,)\n",
      "x_test.shape:  (60000, 28, 28)\n",
      "y_test.shape:  (60000,)\n",
      "x_train.shape:  (50000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Load a toy dataset for the sake of this example\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "print('x_train.shape: ',x_train.shape)\n",
    "print('y_tain.shape: ',y_train.shape)\n",
    "print('x_test.shape: ',x_train.shape)\n",
    "print('y_test.shape: ',y_train.shape)\n",
    "# Preprocess the data (these are Numpy arrays)\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255\n",
    "\n",
    "# Reserve 10,000 samples for validation\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "print('x_train.shape: ',x_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Fit model on training data\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "50000/50000 [==============================] - 10s 191us/sample - loss: 0.3454 - sparse_categorical_accuracy: 0.9026 - val_loss: 0.1798 - val_sparse_categorical_accuracy: 0.9463\n",
      "Epoch 2/3\n",
      "50000/50000 [==============================] - 8s 163us/sample - loss: 0.1591 - sparse_categorical_accuracy: 0.9520 - val_loss: 0.1514 - val_sparse_categorical_accuracy: 0.9556\n",
      "Epoch 3/3\n",
      "50000/50000 [==============================] - 6s 110us/sample - loss: 0.1149 - sparse_categorical_accuracy: 0.9652 - val_loss: 0.1133 - val_sparse_categorical_accuracy: 0.9654\n",
      "\n",
      "history dict: {'loss': [0.3454093938779831, 0.1591050224161148, 0.11487513874411583], 'sparse_categorical_accuracy': [0.9026, 0.952, 0.96522], 'val_loss': [0.1798063812494278, 0.15136583090424538, 0.11329531900882721], 'val_sparse_categorical_accuracy': [0.9463, 0.9556, 0.9654]}\n",
      "\n",
      "# Evaluate on test data\n",
      "10000/10000 [==============================] - 1s 53us/sample - loss: 0.1166 - sparse_categorical_accuracy: 0.9644\n",
      "test loss, test acc: [0.11658219387680292, 0.9644]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions shape: (3, 10)\n"
     ]
    }
   ],
   "source": [
    "# Specify the training configuration (optimizer, loss, metrics)\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(),  # Optimizer\n",
    "              # Loss function to minimize\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "              # List of metrics to monitor\n",
    "              metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "print('# Fit model on training data')\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=3,\n",
    "                    # We pass some validation for\n",
    "                    # monitoring validation loss and metrics\n",
    "                    # at the end of each epoch\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "print('\\nhistory dict:', history.history)\n",
    "\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(x_test[:3])\n",
    "print('predictions shape:', predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying a loss, metrics, and an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**or**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later reuse, let's put our model definition and compile step in functions; we will call them several times across different examples in this guide.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncompiled_model():\n",
    "  inputs = keras.Input(shape=(784,), name='digits')\n",
    "  x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "  x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "  outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "  return model\n",
    "\n",
    "def get_compiled_model():\n",
    "  model = get_uncompiled_model()\n",
    "  model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Many built-in optimizers, losses, and metrics are available\n",
    "- Optimizers: - SGD() (with or without momentum) - RMSprop() - Adam() - etc.\n",
    "\n",
    "- Losses: - MeanSquaredError() - KLDivergence() - CosineSimilarity() - etc.\n",
    "\n",
    "- Metrics: - AUC() - Precision() - Recall() - etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Write custom losses and metrics\n",
    "Create custom metrics by subclassing the **Metric** class. You will need to implement 4 methods:\n",
    "- __init__(self), in which you will create state variables for your metric.\n",
    "\n",
    "- update_state(self, y_true, y_pred, sample_weight=None), which uses the targets y_true and the model predictions y_pred to update the state variables.\n",
    "\n",
    "- result(self), which uses the state variables to compute the final results.\n",
    "\n",
    "- reset_states(self), which reinitializes the state of the metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simple example showing how to implement a `CatgoricalTruePositives` metric, that counts how many samples where correctly classified as belonging to a given class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatgoricalTruePositives(keras.metrics.Metric):\n",
    "  \n",
    "    def __init__(self, name='binary_true_positives', **kwargs):\n",
    "      super(CatgoricalTruePositives, self).__init__(name=name, **kwargs)\n",
    "      self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "      y_pred = tf.argmax(y_pred)\n",
    "      values = tf.equal(tf.cast(y_true, 'int32'), tf.cast(y_pred, 'int32'))\n",
    "      values = tf.cast(values, 'float32')\n",
    "      if sample_weight is not None:\n",
    "        sample_weight = tf.cast(sample_weight, 'float32')\n",
    "        values = tf.multiply(values, sample_weight)\n",
    "      return self.true_positives.assign_add(tf.reduce_sum(values))  # TODO: fix\n",
    "\n",
    "    def result(self):\n",
    "      return tf.identity(self.true_positives)  # TODO: fix\n",
    "    \n",
    "    def reset_states(self):\n",
    "      # The state of the metric will be reset at the start of each epoch.\n",
    "      self.true_positives.assign(0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "50000/50000 [==============================] - 5s 104us/sample - loss: 0.0938 - binary_true_positives: 8460.0000\n",
      "Epoch 2/3\n",
      "50000/50000 [==============================] - 4s 76us/sample - loss: 0.0786 - binary_true_positives: 7986.0000\n",
      "Epoch 3/3\n",
      "50000/50000 [==============================] - 7s 131us/sample - loss: 0.0669 - binary_true_positives: 8286.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1580e5dd748>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[CatgoricalTruePositives()])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling losses and metrics that don't fit the standard signature\n",
    "A regularization loss may only require the activation of a layer (there are no targets in this case), and this activation may not be a model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivityRegularizationLayer(layers.Layer):\n",
    "  \n",
    "  def call(self, inputs):\n",
    "    self.add_loss(tf.reduce_sum(inputs) * 0.1)\n",
    "    return inputs  # Pass-through layer.\n",
    "  \n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "\n",
    "# Insert activity regularization as a layer\n",
    "x = ActivityRegularizationLayer()(x)\n",
    "\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 7s 147us/sample - loss: 2.5209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1580e93bba8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# The displayed loss will be much higher than before\n",
    "# due to the regularization component.\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do the same for **logging metric** values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricLoggingLayer(layers.Layer):\n",
    "  \n",
    "  def call(self, inputs):\n",
    "    # The `aggregation` argument defines\n",
    "    # how to aggregate the per-batch values\n",
    "    # over each epoch:\n",
    "    # in this case we simply average them.\n",
    "    self.add_metric(keras.backend.std(inputs),\n",
    "                    name='std_of_activation',\n",
    "                    aggregation='mean')\n",
    "    return inputs  # Pass-through layer.\n",
    "\n",
    "  \n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "\n",
    "# Insert std logging as a layer.\n",
    "x = MetricLoggingLayer()(x)\n",
    "\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 8s 160us/sample - loss: 0.3297 - std_of_activation: 0.9816s - loss: 0.3377 - std_of_activat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1580feb0668>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss='sparse_categorical_crossentropy')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Functional API**, you can also call `model.add_loss(loss_tensor)`, or `model.add_metric(metric_tensor, name, aggregation)`.\n",
    "\n",
    "Here's a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 6s 122us/sample - loss: 2.4618 - std_of_activation: 0.0017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15813490550>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x1 = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "x2 = layers.Dense(64, activation='relu', name='dense_2')(x1)\n",
    "outputs = layers.Dense(10, activation='softmax', name='predictions')(x2)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.add_loss(tf.reduce_sum(x1) * 0.1)\n",
    "\n",
    "model.add_metric(keras.backend.std(x1),\n",
    "                 name='std_of_activation',\n",
    "                 aggregation='mean')\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "              loss='sparse_categorical_crossentropy')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically setting apart a validation holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "40000/40000 [==============================] - 8s 203us/sample - loss: 0.3744 - sparse_categorical_accuracy: 0.8936 - val_loss: 0.2588 - val_sparse_categorical_accuracy: 0.9231\n",
      "Epoch 2/3\n",
      "40000/40000 [==============================] - 4s 96us/sample - loss: 0.1760 - sparse_categorical_accuracy: 0.9485 - val_loss: 0.1734 - val_sparse_categorical_accuracy: 0.9478\n",
      "Epoch 3/3\n",
      "40000/40000 [==============================] - 2s 62us/sample - loss: 0.1254 - sparse_categorical_accuracy: 0.9624 - val_loss: 0.1657 - val_sparse_categorical_accuracy: 0.9514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1580cfa7128>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "model.fit(x_train, y_train, batch_size=64, validation_split=0.2, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & evaluation from tf.data Datasets\n",
    "Let's now take a look at the case where your data comes in the form of a tf.data Dataset.\n",
    "\n",
    "You can pass a Dataset instance directly to the methods `fit()`, `evaluate()`, and `predict()`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 13s 16ms/step - loss: 0.3262 - sparse_categorical_accuracy: 0.9086 2s - loss: 0.3485 - sparse\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.1562 - sparse_categorical_accuracy: 0.9536 2s - loss: 0.1600 - sparse - ETA: 1s - loss: 0.1565 - spa\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.1156 - sparse_categorical_accuracy: 0.9652\n",
      "\n",
      "# Evaluate\n",
      "157/157 [==============================] - 1s 10ms/step - loss: 0.1461 - sparse_categorical_accuracy: 0.9540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14607782095261393, 0.954]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# First, let's create a training Dataset instance.\n",
    "# For the sake of our example, we'll use the same MNIST data as before.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# Shuffle and slice the dataset.\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Now we get a test dataset.\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(64)\n",
    "\n",
    "# Since the dataset already takes care of batching,\n",
    "# we don't pass a `batch_size` argument.\n",
    "model.fit(train_dataset, epochs=3)\n",
    "\n",
    "# You can also evaluate or predict on a dataset.\n",
    "print('\\n# Evaluate')\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## specific number of batches from this Dataset, you can pass the `steps_per_epoch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "100/100 [==============================] - 4s 44ms/step - loss: 0.7747 - sparse_categorical_accuracy: 0.8031\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 0.3653 - sparse_categorical_accuracy: 0.8961\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 1s 14ms/step - loss: 0.3200 - sparse_categorical_accuracy: 0.9081: 1s - loss: 0.3712 - sparse_cate\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1581414c588>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# Prepare the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Only use the 100 batches per epoch (that's 64 * 100 samples)\n",
    "model.fit(train_dataset, epochs=3, steps_per_epoch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a validation dataset\n",
    "You can pass a Dataset instance as the v`alidation_data` argument in` fit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.3383 - sparse_categorical_accuracy: 0.9039 - val_loss: 0.1904 - val_sparse_categorical_accuracy: 0.9442\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.1571 - sparse_categorical_accuracy: 0.9531 - val_loss: 0.1329 - val_sparse_categorical_accuracy: 0.9609\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.1125 - sparse_categorical_accuracy: 0.9667 - val_loss: 0.1180 - val_sparse_categorical_accuracy: 0.9641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1581414cdd8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# Prepare the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Prepare the validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)\n",
    "\n",
    "model.fit(train_dataset, epochs=3, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify number of batches from this Dataset\n",
    "You can pass the `validation_steps` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.3384 - sparse_categorical_accuracy: 0.9037 - val_loss: 0.2827 - val_sparse_categorical_accuracy: 0.9172y: 0.8 - ETA: 2s - loss: 0.3628 - sparse_cate\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.1609 - sparse_categorical_accuracy: 0.9522 - val_loss: 0.2016 - val_sparse_categorical_accuracy: 0.9453\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.1176 - sparse_categorical_accuracy: 0.9655 - val_loss: 0.1629 - val_sparse_categorical_accuracy: 0.9563\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15816789c88>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# Prepare the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Prepare the validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)\n",
    "\n",
    "model.fit(train_dataset, epochs=3,\n",
    "          # Only run validation using the first 10 batches of the dataset\n",
    "          # using the `validation_steps` argument\n",
    "          validation_data=val_dataset, validation_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sample weighting and class weighting\n",
    "Besides input data and target data, it is possible to pass sample weights or class weights to a model when using fit:\n",
    "\n",
    "- When training from Numpy data: via the `sample_weight` and `class_weight arguments`.\n",
    "- When training from Datasets: by having the Dataset return a tuple (`input_batch`, `target_batch`, s`ample_weight_batch`) .\n",
    "\n",
    "\n",
    "A \"sample weights\" array is an array of numbers that specify how much weight each sample in a batch should have in computing the total loss. It is commonly used in imbalanced classification problems (the idea being to give more weight to rarely-seen classes). When the weights used are ones and zeros, the array can be used as a mask for the loss function (entirely discarding the contribution of certain samples to the total loss).\n",
    "\n",
    "A \"class weights\" dict is a more specific instance of the same concept: it maps class indices to the sample weight that should be used for samples belonging to this class. For instance, if class \"0\" is twice less represented than class \"1\" in your data, you could use `class_weight={0: 1., 1: 0.5}`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Give more importance to the correct classification of class #5 (which is the digit \"5\" in the MNIST dataset).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "50000/50000 [==============================] - 7s 132us/sample - loss: 0.1049 - sparse_categorical_accuracy: 0.97031s - loss: 0.1102 - sparse_categorical_accuracy:  - ETA: 10s - loss: 0\n",
      "Epoch 2/4\n",
      "50000/50000 [==============================] - 4s 87us/sample - loss: 0.0877 - sparse_categorical_accuracy: 0.9754\n",
      "Epoch 3/4\n",
      "50000/50000 [==============================] - 4s 77us/sample - loss: 0.0747 - sparse_categorical_accuracy: 0.9789\n",
      "Epoch 4/4\n",
      "50000/50000 [==============================] - 5s 106us/sample - loss: 0.0653 - sparse_categorical_accuracy: 0.9818\n",
      "Epoch 1/4\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 0.3738 - sparse_categorical_accuracy: 0.8998s - loss: 0.6396 - spars\n",
      "Epoch 2/4\n",
      "50000/50000 [==============================] - 4s 83us/sample - loss: 0.1700 - sparse_categorical_accuracy: 0.9523\n",
      "Epoch 3/4\n",
      "50000/50000 [==============================] - 4s 84us/sample - loss: 0.1243 - sparse_categorical_accuracy: 0.9651\n",
      "Epoch 4/4\n",
      "50000/50000 [==============================] - 6s 118us/sample - loss: 0.0996 - sparse_categorical_accuracy: 0.9728s - loss: 0.099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15818684da0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class_weight = {0: 1., 1: 1., 2: 1., 3: 1., 4: 1.,\n",
    "                # Set weight \"2\" for class \"5\",\n",
    "                # making this class 2x more important\n",
    "                5: 2.,\n",
    "                6: 1., 7: 1., 8: 1., 9: 1.}\n",
    "model.fit(x_train, y_train,\n",
    "          class_weight=class_weight,\n",
    "          batch_size=64,\n",
    "          epochs=4)\n",
    "\n",
    "# Here's the same example using `sample_weight` instead:\n",
    "sample_weight = np.ones(shape=(len(y_train),))\n",
    "sample_weight[y_train == 5] = 2.\n",
    "\n",
    "model = get_compiled_model()\n",
    "model.fit(x_train, y_train,\n",
    "          sample_weight=sample_weight,\n",
    "          batch_size=64,\n",
    "          epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a matching Dataset example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3767 - sparse_categorical_accuracy: 0.9020 1s - loss: 0.3899 - sparse_categorical_ac\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.1779 - sparse_categorical_accuracy: 0.9509\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.1288 - sparse_categorical_accuracy: 0.9645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x158180f6470>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weight = np.ones(shape=(len(y_train),))\n",
    "sample_weight[y_train == 5] = 2.\n",
    "\n",
    "# Create a Dataset that includes sample weights\n",
    "# (3rd element in the return tuple).\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train, sample_weight))\n",
    "\n",
    "# Shuffle and slice the dataset.\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "model = get_compiled_model()\n",
    "model.fit(train_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing data to multi-input, multi-output models\n",
    "\n",
    "Consider the following model, which has an image input of shape `(32, 32, 3)` (that's `(height, width, channels)`) and a timeseries input of shape `(None, 10)` (that's `(timesteps, features)`). Our model will have two outputs computed from the combination of these `inputs: a \"score\"` (of shape `(1,)`) and a probability distribution over 5 classes (of shape `(10,)`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "image_input = keras.Input(shape=(32, 32, 3), name='img_input')\n",
    "timeseries_input = keras.Input(shape=(None, 10), name='ts_input')\n",
    "\n",
    "x1 = layers.Conv2D(3, 3)(image_input)\n",
    "x1 = layers.GlobalMaxPooling2D()(x1)\n",
    "\n",
    "x2 = layers.Conv1D(3, 3)(timeseries_input)\n",
    "x2 = layers.GlobalMaxPooling1D()(x2)\n",
    "\n",
    "x = layers.concatenate([x1, x2])\n",
    "\n",
    "score_output = layers.Dense(1, name='score_output')(x)\n",
    "class_output = layers.Dense(5, activation='softmax', name='class_output')(x)\n",
    "\n",
    "model = keras.Model(inputs=[image_input, timeseries_input],\n",
    "                    outputs=[score_output, class_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At **compilation time**, we can specify different losses to different ouptuts, by passing the loss functions as a list:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[keras.losses.MeanSquaredError(),\n",
    "          keras.losses.CategoricalCrossentropy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise for metrics:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[keras.losses.MeanSquaredError(),\n",
    "          keras.losses.CategoricalCrossentropy()],\n",
    "    metrics=[[keras.metrics.MeanAbsolutePercentageError(),\n",
    "              keras.metrics.MeanAbsoluteError()],\n",
    "             [keras.metrics.CategoricalAccuracy()]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or Since we gave names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={'score_output': keras.losses.MeanSquaredError(),\n",
    "          'class_output': keras.losses.CategoricalCrossentropy()},\n",
    "    metrics={'score_output': [keras.metrics.MeanAbsolutePercentageError(),\n",
    "                              keras.metrics.MeanAbsoluteError()],\n",
    "             'class_output': [keras.metrics.CategoricalAccuracy()]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend the use of explicit names and dicts if you have more than 2 outputs.\n",
    " \n",
    " One might wish to privilege the \"score\" loss in our example, by giving to 2x the importance of the class loss, using the `loss_weight` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={'score_output': keras.losses.MeanSquaredError(),\n",
    "          'class_output': keras.losses.CategoricalCrossentropy()},\n",
    "    metrics={'score_output': [keras.metrics.MeanAbsolutePercentageError(),\n",
    "                              keras.metrics.MeanAbsoluteError()],\n",
    "             'class_output': [keras.metrics.CategoricalAccuracy()]},\n",
    "    loss_weight={'score_output': 2., 'class_output': 1.})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose not to compute a loss for certain outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0320 22:57:44.684697 29612 training_utils.py:1152] Output score_output missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to score_output.\n"
     ]
    }
   ],
   "source": [
    "# List loss version\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[None, keras.losses.CategoricalCrossentropy()])\n",
    "\n",
    "# Or dict loss version\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={'class_output': keras.losses.CategoricalCrossentropy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "100/100 [==============================] - 2s 16ms/sample - loss: 4.9293 - score_output_loss: 0.4086 - class_output_loss: 4.5207\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 4.7968 - score_output_loss: 0.2626 - class_output_loss: 4.5343\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 4.7240 - score_output_loss: 0.2024 - class_output_loss: 4.5215\n",
      "Epoch 1/3\n",
      "100/100 [==============================] - 1s 11ms/sample - loss: 4.6530 - score_output_loss: 0.1610 - class_output_loss: 4.4920\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 1s 7ms/sample - loss: 4.5398 - score_output_loss: 0.1476 - class_output_loss: 4.3922\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 0s 5ms/sample - loss: 4.4614 - score_output_loss: 0.1405 - class_output_loss: 4.3209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1581a3498d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[keras.losses.MeanSquaredError(),\n",
    "          keras.losses.CategoricalCrossentropy()])\n",
    "\n",
    "# Generate dummy Numpy data\n",
    "img_data = np.random.random_sample(size=(100, 32, 32, 3))\n",
    "ts_data = np.random.random_sample(size=(100, 20, 10))\n",
    "score_targets = np.random.random_sample(size=(100, 1))\n",
    "class_targets = np.random.random_sample(size=(100, 5))\n",
    "\n",
    "# Fit on lists\n",
    "model.fit([img_data, ts_data], [score_targets, class_targets],\n",
    "          batch_size=32,\n",
    "          epochs=3)\n",
    "\n",
    "# Alernatively, fit on dicts\n",
    "model.fit({'img_input': img_data, 'ts_input': ts_data},\n",
    "          {'score_output': score_targets, 'class_output': class_targets},\n",
    "          batch_size=32,\n",
    "          epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset use case: similarly as what we did for Numpy arrays, the Dataset should return a tuple of dicts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2/2 [==============================] - 1s 368ms/step - loss: 4.4123 - score_output_loss: 0.1311 - class_output_loss: 4.2826\n",
      "Epoch 2/3\n",
      "2/2 [==============================] - 1s 331ms/step - loss: 4.3894 - score_output_loss: 0.1269 - class_output_loss: 4.2636\n",
      "Epoch 3/3\n",
      "2/2 [==============================] - 1s 269ms/step - loss: 4.3687 - score_output_loss: 0.1234 - class_output_loss: 4.2463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x158180f6ba8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ({'img_input': img_data, 'ts_input': ts_data},\n",
    "     {'score_output': score_targets, 'class_output': class_targets}))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "model.fit(train_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using callbacks\n",
    "Callbacks in Keras are objects that are called at different point during training (at the start of an epoch, at the end of a batch, at the end of an epoch, etc.) and which can be used to implement behaviors such as:\n",
    "\n",
    "- Doing validation at different points during training (beyond the built-in per-epoch validation)\n",
    "- Checkpointing the model at regular intervals or when it exceeds a certain accuracy threshold\n",
    "- Changing the learning rate of the model when training seems to be plateauing\n",
    "- Doing fine-tuning of the top layers when training seems to be plateauing\n",
    "- Sending email or instant message notifications when training ends or where a certain performance threshold is exceeded\n",
    "- Etc.\n",
    "\n",
    "Callbacks can be passed as a list to your call to `fit`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 8s 188us/sample - loss: 0.3701 - sparse_categorical_accuracy: 0.8949 - val_loss: 0.2246 - val_sparse_categorical_accuracy: 0.9337\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 6s 140us/sample - loss: 0.1737 - sparse_categorical_accuracy: 0.9484 - val_loss: 0.2011 - val_sparse_categorical_accuracy: 0.9406\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 6s 162us/sample - loss: 0.1265 - sparse_categorical_accuracy: 0.9621 - val_loss: 0.1536 - val_sparse_categorical_accuracy: 0.9546\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 4s 109us/sample - loss: 0.1008 - sparse_categorical_accuracy: 0.9696 - val_loss: 0.1632 - val_sparse_categorical_accuracy: 0.9529\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 3s 84us/sample - loss: 0.0822 - sparse_categorical_accuracy: 0.9741 - val_loss: 0.1427 - val_sparse_categorical_accuracy: 0.9611\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 4s 91us/sample - loss: 0.0685 - sparse_categorical_accuracy: 0.9788 - val_loss: 0.1425 - val_sparse_categorical_accuracy: 0.9588\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 7s 165us/sample - loss: 0.0590 - sparse_categorical_accuracy: 0.9817 - val_loss: 0.1382 - val_sparse_categorical_accuracy: 0.9629\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1582a3a7fd0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        # Stop training when `val_loss` is no longer improving\n",
    "        monitor='val_loss',\n",
    "        # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "        min_delta=1e-2,\n",
    "        # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
    "        patience=2,\n",
    "        verbose=1)\n",
    "]\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=64,\n",
    "          callbacks=callbacks,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Many built-in callbacks\n",
    "- **ModelCheckpoint**: Periodically save the model.\n",
    "- **EarlyStopping**: Stop training when training is no longer improving the validation metrics.\n",
    "- **TensorBoard**: periodically write model logs that can be visualized in TensorBoard (more details in the section \"Visualization\").\n",
    "- **CSVLogger**: streams loss and metrics data to a CSV file.\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Write your own Callback\n",
    "Create a custom callback by extending the base class `keras.callbacks.Callback`.\n",
    "\n",
    "Access to its associated model through the class property `self.model`.\n",
    "\n",
    "**Simple Example**: Saving a list of per-batch loss values during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "\n",
    "    def on_train_begin(self, logs):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.losses.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpointing models\n",
    "When you're training model on relatively large datasets, it's crucial to save checkpoints of your model at frequent intervals.\n",
    "\n",
    "The easiest way to achieve this is with the ModelCheckpoint callback:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "39488/40000 [============================>.] - ETA: 0s - loss: 0.3659 - sparse_categorical_accuracy: 0.8975\n",
      "Epoch 00001: val_loss improved from inf to 0.23448, saving model to mymodel_1.h5\n",
      "40000/40000 [==============================] - 8s 209us/sample - loss: 0.3638 - sparse_categorical_accuracy: 0.8980 - val_loss: 0.2345 - val_sparse_categorical_accuracy: 0.9295\n",
      "Epoch 2/3\n",
      "39808/40000 [============================>.] - ETA: 0s - loss: 0.1690 - sparse_categorical_accuracy: 0.9508\n",
      "Epoch 00002: val_loss improved from 0.23448 to 0.17551, saving model to mymodel_2.h5\n",
      "40000/40000 [==============================] - 6s 153us/sample - loss: 0.1691 - sparse_categorical_accuracy: 0.9508 - val_loss: 0.1755 - val_sparse_categorical_accuracy: 0.9483\n",
      "Epoch 3/3\n",
      "39936/40000 [============================>.] - ETA: 0s - loss: 0.1218 - sparse_categorical_accuracy: 0.9644\n",
      "Epoch 00003: val_loss improved from 0.17551 to 0.16445, saving model to mymodel_3.h5\n",
      "40000/40000 [==============================] - 6s 140us/sample - loss: 0.1217 - sparse_categorical_accuracy: 0.9644 - val_loss: 0.1645 - val_sparse_categorical_accuracy: 0.9540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1582aafc860>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='mymodel_{epoch}.h5',\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=3,\n",
    "          batch_size=64,\n",
    "          callbacks=callbacks,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### You call also write your own callback for saving and restoring models.\n",
    "\n",
    "For a complete guide on serialization and saving, see Guide to Saving and Serializing Models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using learning rate schedules\n",
    "A common pattern when training deep learning models is to gradually reduce the learning as training progresses. This is generally known as \"learning rate decay\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passing a schedule to an optimizer\n",
    "static learning rate decay schedule by passing a schedule object as the learning_rate argument in your optimizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.1\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several built-in schedules are available: ExponentialDecay, PiecewiseConstantDecay, PolynomialDecay, and InverseTimeDecay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dynamic learning rate schedule\n",
    "A dynamic learning rate schedule (for instance, decreasing the learning rate when the validation loss is no longer improving) cannot be achieved with these schedule objects since the optimizer does not have access to validation metrics.\n",
    "\n",
    "However, callbacks do have access to all metrics, including validation metrics! You can thus achieve this pattern by using a callback that modifies the current learning rate on the optimizer. In fact, this is even built-in as the ReduceLROnPlateau callback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing loss and metrics during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-35-d76f85f8d4f6>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-35-d76f85f8d4f6>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    tensorboard --logdir=/full_path_to_your_logs\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "tensorboard --logdir=/full_path_to_your_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the TensorBoard callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-c13d02c02252>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtensorboard_cbk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'/full_path_to_your_logs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard_cbk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "tensorboard_cbk = keras.callbacks.TensorBoard(log_dir='/full_path_to_your_logs')\n",
    "model.fit(dataset, epochs=10, callbacks=[tensorboard_cbk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.TensorBoard at 0x1582740bc18>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.callbacks.TensorBoard(\n",
    "  log_dir='/full_path_to_your_logs',\n",
    "  histogram_freq=0,  # How often to log histogram visualizations\n",
    "  embeddings_freq=0,  # How often to log embedding visualizations\n",
    "  update_freq='epoch')  # How often to write logs (default: once per epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Writing your own training & evaluation loops from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the GradientTape: a first end-to-end example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 2.416548728942871\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 2.24124813079834\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 2.148831367492676\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 2.1140458583831787\n",
      "Seen so far: 38464 samples\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 2.0647637844085693\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 1.876824975013733\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 1.7586379051208496\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 1.7045397758483887\n",
      "Seen so far: 38464 samples\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 1.609616994857788\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 1.4744676351547241\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 1.3040378093719482\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 1.2615548372268677\n",
      "Seen so far: 38464 samples\n"
     ]
    }
   ],
   "source": [
    "# Get the model.\n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Prepare the training dataset.\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Iterate over epochs.\n",
    "for epoch in range(3):\n",
    "  print('Start of epoch %d' % (epoch,))\n",
    "  \n",
    "  # Iterate over the batches of the dataset.\n",
    "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "\n",
    "    # Open a GradientTape to record the operations run\n",
    "    # during the forward pass, which enables autodifferentiation.\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "      # Run the forward pass of the layer.\n",
    "      # The operations that the layer applies\n",
    "      # to its inputs are going to be recorded\n",
    "      # on the GradientTape.\n",
    "      logits = model(x_batch_train)  # Logits for this minibatch\n",
    "\n",
    "      # Compute the loss value for this minibatch.\n",
    "      loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "      # Use the gradient tape to automatically retrieve\n",
    "      # the gradients of the trainable weights with respect to the loss.\n",
    "      grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "      # Run one step of gradient descent by updating\n",
    "      # the value of the weights to minimize the loss.\n",
    "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Log every 200 batches.\n",
    "    if step % 200 == 0:\n",
    "        print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
    "        print('Seen so far: %s samples' % ((step + 1) * 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-level handling of metrics\n",
    "Let's add metrics to the mix. You can readily reuse the built-in metrics (or custom ones you wrote) in such training loops written from scratch. Here's the flow:\n",
    "\n",
    "- Instantiate the metric at the start of the loop\n",
    "- Call metric.update_state() after each batch\n",
    "- Call metric.result() when you need to display the current value of the metric\n",
    "- Call metric.reset_states() when you need to clear the state of the metric (typically at the end of an epoch)\n",
    "\n",
    "\n",
    "\n",
    "Let's use this knowledge to compute SparseCategoricalAccuracy on validation data at the end of each epoch:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 2.266350269317627\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 2.2238714694976807\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 2.2236430644989014\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 2.049403667449951\n",
      "Seen so far: 38464 samples\n",
      "Training acc over epoch: 0.2945399880409241\n",
      "Validation acc: 0.45820000767707825\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 1.8961845636367798\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 1.8781015872955322\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 1.892068862915039\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 1.605812430381775\n",
      "Seen so far: 38464 samples\n",
      "Training acc over epoch: 0.5465199947357178\n",
      "Validation acc: 0.6521999835968018\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 1.4581108093261719\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 1.4652509689331055\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 1.5118515491485596\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 1.2108690738677979\n",
      "Seen so far: 38464 samples\n",
      "Training acc over epoch: 0.6779400110244751\n",
      "Validation acc: 0.7455000281333923\n"
     ]
    }
   ],
   "source": [
    "# Get model\n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Instantiate an optimizer to train the model.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Prepare the metrics.\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy() \n",
    "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# Prepare the training dataset.\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)\n",
    "\n",
    "\n",
    "# Iterate over epochs.\n",
    "for epoch in range(3):\n",
    "  print('Start of epoch %d' % (epoch,))\n",
    "  \n",
    "  # Iterate over the batches of the dataset.\n",
    "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "      logits = model(x_batch_train)\n",
    "      loss_value = loss_fn(y_batch_train, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "      \n",
    "    # Update training metric.\n",
    "    train_acc_metric(y_batch_train, logits)\n",
    "\n",
    "    # Log every 200 batches.\n",
    "    if step % 200 == 0:\n",
    "        print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
    "        print('Seen so far: %s samples' % ((step + 1) * 64))\n",
    "\n",
    "  # Display metrics at the end of each epoch.\n",
    "  train_acc = train_acc_metric.result()\n",
    "  print('Training acc over epoch: %s' % (float(train_acc),))\n",
    "  # Reset training metrics at the end of each epoch\n",
    "  train_acc_metric.reset_states()\n",
    "\n",
    "  # Run a validation loop at the end of each epoch.\n",
    "  for x_batch_val, y_batch_val in val_dataset:\n",
    "    val_logits = model(x_batch_val)\n",
    "    # Update val metrics\n",
    "    val_acc_metric(y_batch_val, val_logits)\n",
    "  val_acc = val_acc_metric.result()\n",
    "  val_acc_metric.reset_states()\n",
    "  print('Validation acc: %s' % (float(val_acc),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-level handling of extra losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivityRegularizationLayer(layers.Layer):\n",
    "  \n",
    "  def call(self, inputs):\n",
    "    self.add_loss(1e-2 * tf.reduce_sum(inputs))\n",
    "    return inputs\n",
    "  \n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "# Insert activity regularization as a layer\n",
    "x = ActivityRegularizationLayer()(x)\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you call a model, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=1005933, shape=(), dtype=float32, numpy=6.8379745>]\n"
     ]
    }
   ],
   "source": [
    "logits = model(x_train)\n",
    "# the losses it creates during the forward pass are added to the model.losses attribute:\n",
    "logits = model(x_train[:64])\n",
    "print(model.losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=1005994, shape=(), dtype=float32, numpy=6.6601048>]\n"
     ]
    }
   ],
   "source": [
    "logits = model(x_train[:64])\n",
    "logits = model(x_train[64: 128])\n",
    "logits = model(x_train[128: 192])\n",
    "print(model.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take these losses into account during training, all you have to do is to modify your training loop to add sum(model.losses) to your total loss:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 9.203619956970215\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 2.4950966835021973\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 2.383124351501465\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 2.3560397624969482\n",
      "Seen so far: 38464 samples\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 2.3431715965270996\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 2.328341484069824\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 2.3178470134735107\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 2.3198299407958984\n",
      "Seen so far: 38464 samples\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 2.3188881874084473\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 2.3141603469848633\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 2.307861566543579\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 2.31400465965271\n",
      "Seen so far: 38464 samples\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "for epoch in range(3):\n",
    "  print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "      logits = model(x_batch_train)\n",
    "      loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "      # Add extra losses created during this forward pass:\n",
    "      loss_value += sum(model.losses)\n",
    "      \n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Log every 200 batches.\n",
    "    if step % 200 == 0:\n",
    "        print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
    "        print('Seen so far: %s samples' % ((step + 1) * 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
