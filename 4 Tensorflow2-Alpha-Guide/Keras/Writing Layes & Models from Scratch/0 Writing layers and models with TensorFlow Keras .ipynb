{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing layers and models with TensorFlow Keras\n",
    "Source: https://www.tensorflow.org/alpha/guide/keras/custom_layers_and_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Layer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers encapsulate a state (weights) and some computation\n",
    "Here's a densely-connected layer. It has a state: the variables w and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.10136678  0.05350842  0.14364123 -0.04548384]\n",
      " [-0.10136678  0.05350842  0.14364123 -0.04548384]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class Linear(layers.Layer):\n",
    "\n",
    "  def __init__(self, units=32, input_dim=32):\n",
    "    super(Linear, self).__init__()\n",
    "    w_init = tf.random_normal_initializer()\n",
    "    self.w = tf.Variable(initial_value=w_init(shape=(input_dim, units),\n",
    "                                              dtype='float32'),\n",
    "                         trainable=True)\n",
    "    b_init = tf.zeros_initializer()\n",
    "    self.b = tf.Variable(initial_value=b_init(shape=(units,),\n",
    "                                              dtype='float32'),\n",
    "                         trainable=True)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "x = tf.ones((2, 2))\n",
    "linear_layer = Linear(4, 2)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the weights w and b are automatically tracked by the layer upon being set as layer attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also have access to a quicker shortcut for adding weight to a layer: the `add_weight` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.03995258 -0.04885546  0.03693784  0.12297569]\n",
      " [-0.03995258 -0.04885546  0.03693784  0.12297569]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class Linear(layers.Layer):\n",
    "\n",
    "  def __init__(self, units=32, input_dim=32):\n",
    "    super(Linear, self).__init__()\n",
    "    self.w = self.add_weight(shape=(input_dim, units),\n",
    "                             initializer='random_normal',\n",
    "                             trainable=True)\n",
    "    self.b = self.add_weight(shape=(units,),\n",
    "                             initializer='zeros',\n",
    "                             trainable=True)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "x = tf.ones((2, 2))\n",
    "linear_layer = Linear(4, 2)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers can have non-trainable weights\n",
    "Besides trainable weights, you can add non-trainable weights to a layer as well. Such weights are meant not to be taken into account during backpropagation, when you are training the layer.\n",
    "\n",
    "Here's how to add and use a non-trainable weight:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2.]\n",
      "[4. 4.]\n"
     ]
    }
   ],
   "source": [
    "class ComputeSum(layers.Layer):\n",
    "\n",
    "  def __init__(self, input_dim):\n",
    "    super(ComputeSum, self).__init__()\n",
    "    self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),\n",
    "                             trainable=False)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "    return self.total  \n",
    "\n",
    "x = tf.ones((2, 2))\n",
    "my_sum = ComputeSum(2)\n",
    "y = my_sum(x)\n",
    "print(y.numpy())\n",
    "y = my_sum(x)\n",
    "print(y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's part of layer.weights, but it gets categorized as a non-trainable weight:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 1\n",
      "non-trainable weights: 1\n",
      "trainable_weights: []\n"
     ]
    }
   ],
   "source": [
    "print('weights:', len(my_sum.weights))\n",
    "print('non-trainable weights:', len(my_sum.non_trainable_weights))\n",
    "\n",
    "# It's not included in the trainable weights:\n",
    "print('trainable_weights:', my_sum.trainable_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best practice: deferring weight creation until the shape of the inputs is known\n",
    "In the logistic regression example above, our Linear layer took an input_dim argument that was used to compute the shape of the weights w and b in` __init__`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(layers.Layer):\n",
    "\n",
    "  def __init__(self, units=32, input_dim=32):\n",
    "      super(Linear, self).__init__()\n",
    "      self.w = self.add_weight(shape=(input_dim, units),\n",
    "                               initializer='random_normal',\n",
    "                               trainable=True)\n",
    "      self.b = self.add_weight(shape=(units,),\n",
    "                               initializer='random_normal',\n",
    "                               trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, you may not know in advance the size of your inputs, and you would like to lazily create weights when that value becomes known, some time after instantiating the layer.\n",
    "\n",
    "In the Keras API, we recommend creating layer weights in the build(inputs_shape) method of your layer. Like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(layers.Layer):\n",
    "\n",
    "  def __init__(self, units=32):\n",
    "    super(Linear, self).__init__()\n",
    "    self.units = units\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                             initializer='random_normal',\n",
    "                             trainable=True)\n",
    "    self.b = self.add_weight(shape=(self.units,),\n",
    "                             initializer='random_normal',\n",
    "                             trainable=True)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __call__ method of your layer will automatically run build the first time it is called. You now have a layer that's lazy and easy to use:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = Linear(32)  # At instantiation, we don't know on what inputs this is going to get called\n",
    "y = linear_layer(x)  # The layer's weights are created dynamically the first time the layer is called"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you assign a Layer instance as attribute of another Layer, the outer layer will start tracking the weights of the inner layer.\n",
    "\n",
    "We recommend creating such sublayers in the __init__ method (since the sublayers will typically have a build method, they will be built when the outer layer gets built).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 6\n",
      "trainable weights: 6\n"
     ]
    }
   ],
   "source": [
    "# Let's assume we are reusing the Linear class\n",
    "# with a `build` method that we defined above.\n",
    "\n",
    "class MLPBlock(layers.Layer):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(MLPBlock, self).__init__()\n",
    "    self.linear_1 = Linear(32)\n",
    "    self.linear_2 = Linear(32)\n",
    "    self.linear_3 = Linear(1)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = self.linear_1(inputs)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = self.linear_2(x)\n",
    "    x = tf.nn.relu(x)\n",
    "    return self.linear_3(x)\n",
    "      \n",
    "\n",
    "mlp = MLPBlock()\n",
    "y = mlp(tf.ones(shape=(3, 64)))  # The first call to the `mlp` will create the weights\n",
    "print('weights:', len(mlp.weights))\n",
    "print('trainable weights:', len(mlp.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers recursively collect losses created during the forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When writing the call method of a layer, you can create loss tensors that you will want to use later, when writing your training loop. This is doable by calling `self.add_loss(value)`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A layer that creates an activity regularization loss\n",
    "class ActivityRegularizationLayer(layers.Layer):\n",
    "  \n",
    "  def __init__(self, rate=1e-2):\n",
    "    super(ActivityRegularizationLayer, self).__init__()\n",
    "    self.rate = rate\n",
    "  \n",
    "  def call(self, inputs):\n",
    "    self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OuterLayer(layers.Layer):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(OuterLayer, self).__init__()\n",
    "    self.activity_reg = ActivityRegularizationLayer(1e-2)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return self.activity_reg(inputs)\n",
    "\n",
    "\n",
    "layer = OuterLayer()\n",
    "assert len(layer.losses) == 0  # No losses yet since the layer has never been called\n",
    "_ = layer(tf.zeros(1, 1))\n",
    "assert len(layer.losses) == 1  # We created one loss value\n",
    "\n",
    "# `layer.losses` gets reset at the start of each __call__\n",
    "_ = layer(tf.zeros(1, 1))\n",
    "assert len(layer.losses) == 1  # This is the loss created during the call above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=249, shape=(), dtype=float32, numpy=0.0015966543>]\n"
     ]
    }
   ],
   "source": [
    "class OuterLayer(layers.Layer):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(OuterLayer, self).__init__()\n",
    "    self.dense = layers.Dense(32, kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    return self.dense(inputs)\n",
    "  \n",
    "  \n",
    "layer = OuterLayer()\n",
    "_ = layer(tf.zeros((1, 1)))\n",
    "\n",
    "# This is `1e-3 * sum(layer.dense.kernel)`,\n",
    "# created by the `kernel_regularizer` above.\n",
    "print(layer.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-1c66c8f97690>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-17-1c66c8f97690>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    loss_value = loss_fn(y_batch_train, logits))\u001b[0m\n\u001b[1;37m                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Instantiate an optimizer.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Iterate over the batches of a dataset.\n",
    "for x_batch_train, y_batch_train in train_dataset:\n",
    "  with tf.GradientTape() as tape:\n",
    "    logits = layer(x_batch_train)  # Logits for this minibatch\n",
    "    # Loss value for this minibatch\n",
    "    loss_value = loss_fn(y_batch_train, logits))\n",
    "    # Add extra losses created during this forward pass:\n",
    "    loss_value += sum(model.losses)\n",
    "\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Models\n",
    "## The Model class\n",
    "\n",
    "\n",
    "class ResNet(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.block_1 = ResNetBlock()\n",
    "        self.block_2 = ResNetBlock()\n",
    "        self.global_pool = layers.GlobalAveragePooling2D()\n",
    "        self.classifier = Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.block_1(inputs)\n",
    "        x = self.block_2(x)\n",
    "        x = self.global_pool(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "resnet = ResNet()\n",
    "\n",
    "dataset = ...\n",
    "\n",
    "resnet.fit(dataset, epochs=10)\n",
    "\n",
    "resnet.save_weights(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together: an end-to-end example\n",
    "**Here's what you've learned so far:** \n",
    "- A Layer encapsulate a state (created in __init__ or build) and some computation (in call). \n",
    "- Layers can be recursively nested to create new, bigger computation blocks. \n",
    "- Layers can create and track losses (typically regularization losses). \n",
    "- The outer container, the thing you want to train, is a Model. A Model is just like a Layer, but with added training and serialization utilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End-to-end example: we're going to implement a \n",
    "## Variational AutoEncoder (VAE)\n",
    "We'll train it on MNIST digits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "step 0: mean loss = tf.Tensor(169.26942, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(5.2743306, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(2.698503, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(1.8305213, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(1.3932781, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(1.1302196, shape=(), dtype=float32)\n",
      "step 600: mean loss = tf.Tensor(0.9545002, shape=(), dtype=float32)\n",
      "step 700: mean loss = tf.Tensor(0.82864803, shape=(), dtype=float32)\n",
      "step 800: mean loss = tf.Tensor(0.7341983, shape=(), dtype=float32)\n",
      "step 900: mean loss = tf.Tensor(0.6604276, shape=(), dtype=float32)\n",
      "Start of epoch 1\n",
      "step 0: mean loss = tf.Tensor(0.63648486, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(0.5819461, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(0.5369934, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(0.49927488, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class Sampling(layers.Layer):\n",
    "  \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "  def call(self, inputs):\n",
    "    z_mean, z_log_var = inputs\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "  \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"\n",
    "  \n",
    "  def __init__(self,\n",
    "               latent_dim=32,\n",
    "               intermediate_dim=64,\n",
    "               name='encoder',\n",
    "               **kwargs):\n",
    "    super(Encoder, self).__init__(name=name, **kwargs)\n",
    "    self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\n",
    "    self.dense_mean = layers.Dense(latent_dim)\n",
    "    self.dense_log_var = layers.Dense(latent_dim)\n",
    "    self.sampling = Sampling()\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = self.dense_proj(inputs)\n",
    "    z_mean = self.dense_mean(x)\n",
    "    z_log_var = self.dense_log_var(x)\n",
    "    z = self.sampling((z_mean, z_log_var))\n",
    "    return z_mean, z_log_var, z\n",
    "\n",
    "  \n",
    "class Decoder(layers.Layer):\n",
    "  \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n",
    "  \n",
    "  def __init__(self,\n",
    "               original_dim,\n",
    "               intermediate_dim=64,\n",
    "               name='decoder',\n",
    "               **kwargs):\n",
    "    super(Decoder, self).__init__(name=name, **kwargs)\n",
    "    self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\n",
    "    self.dense_output = layers.Dense(original_dim, activation='sigmoid')\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    x = self.dense_proj(inputs)\n",
    "    return self.dense_output(x)\n",
    "\n",
    "\n",
    "class VariationalAutoEncoder(tf.keras.Model):\n",
    "  \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
    "  \n",
    "  def __init__(self,\n",
    "               original_dim,\n",
    "               intermediate_dim=64,\n",
    "               latent_dim=32,\n",
    "               name='autoencoder',\n",
    "               **kwargs):\n",
    "    super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
    "    self.original_dim = original_dim\n",
    "    self.encoder = Encoder(latent_dim=latent_dim,\n",
    "                           intermediate_dim=intermediate_dim)\n",
    "    self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    z_mean, z_log_var, z = self.encoder(inputs)\n",
    "    reconstructed = self.decoder(z)\n",
    "    # Add KL divergence regularization loss.\n",
    "    kl_loss = - 0.5 * tf.reduce_sum(\n",
    "        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "    self.add_loss(kl_loss)\n",
    "    return reconstructed\n",
    "\n",
    "\n",
    "original_dim = 784\n",
    "vae = VariationalAutoEncoder(original_dim, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Iterate over epochs.\n",
    "for epoch in range(3):\n",
    "  print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "  # Iterate over the batches of the dataset.\n",
    "  for step, x_batch_train in enumerate(train_dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "      reconstructed = vae(x_batch_train)\n",
    "      # Compute reconstruction loss\n",
    "      loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "      loss += sum(vae.losses)  # Add KLD regularization loss\n",
    "      \n",
    "    grads = tape.gradient(loss, vae.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, vae.trainable_variables))\n",
    "    \n",
    "    loss_metric(loss)\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "      print('step %s: mean loss = %s' % (step, loss_metric.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since the VAE is subclassing Model, it features built-in training loops. So you could also have trained it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VariationalAutoEncoder(784, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(x_train, x_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond object-oriented development: the Functional API\n",
    "For instance, the Functional API example below reuses the same Sampling layer we defined in the example above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = 784\n",
    "intermediate_dim = 64\n",
    "latent_dim = 32\n",
    "\n",
    "# Define encoder model.\n",
    "original_inputs = tf.keras.Input(shape=(original_dim,), name='encoder_input')\n",
    "x = layers.Dense(intermediate_dim, activation='relu')(original_inputs)\n",
    "z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "z = Sampling()((z_mean, z_log_var))\n",
    "encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name='encoder')\n",
    "\n",
    "# Define decoder model.\n",
    "latent_inputs = tf.keras.Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = layers.Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = layers.Dense(original_dim, activation='sigmoid')(x)\n",
    "decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name='decoder')\n",
    "\n",
    "# Define VAE model.\n",
    "outputs = decoder(z)\n",
    "vae = tf.keras.Model(inputs=original_inputs, outputs=outputs, name='vae')\n",
    "\n",
    "# Add KL divergence regularization loss.\n",
    "kl_loss = - 0.5 * tf.reduce_sum(\n",
    "    z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "vae.add_loss(kl_loss)\n",
    "\n",
    "# Train.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(x_train, x_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
